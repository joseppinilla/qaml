{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Quantum RBM training on the OptDigits Dataset for reconstruction and classification\n",
        "This is an example on quantum annealing training of an RBM on the OptDigits\n",
        "dataset.\n",
        "Developed by: Jose Pinilla"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Required packages\n",
        "import qaml\n",
        "import torch\n",
        "torch.manual_seed(0) # For deterministic weights\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as torch_transforms\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################# Hyperparameters ##############################\n",
        "M,N = SHAPE = (8,8)\n",
        "EPOCHS = 75\n",
        "BATCH_SIZE = 1024\n",
        "SUBCLASSES = [1,2,3,4]\n",
        "DATA_SIZE = N*M\n",
        "LABEL_SIZE = len(SUBCLASSES)\n",
        "# Stochastic Gradient Descent\n",
        "learning_rate = 0.1\n",
        "weight_decay = 1e-4\n",
        "momentum = 0.5\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#################################### Input Data ################################\n",
        "train_dataset = qaml.datasets.OptDigits(root='./data/', train=True,\n",
        "                                     transform=torch_transforms.Compose([\n",
        "                                     torch_transforms.ToTensor(),\n",
        "                                     lambda x:(x>0.5).to(x.dtype)]), #Binarize\n",
        "                                     target_transform=torch_transforms.Compose([\n",
        "                                     lambda x:torch.LongTensor([x.astype(int)]),\n",
        "                                     lambda x:F.one_hot(x-1,len(SUBCLASSES))]),\n",
        "                                     download=True)\n",
        "\n",
        "train_idx = [i for i,y in enumerate(train_dataset.targets) if y in SUBCLASSES]\n",
        "sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                                           sampler=sampler)\n",
        "\n",
        "fig,axs = plt.subplots(4,5)\n",
        "subdataset = zip(train_dataset.data[train_idx],train_dataset.targets[train_idx])\n",
        "for ax,(img,label) in zip(axs.flat,subdataset):\n",
        "    ax.matshow(img>0.5)\n",
        "    ax.set_title(int(label))\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "\n",
        "test_dataset = qaml.datasets.OptDigits(root='./data/', train=False,\n",
        "                                    transform=torch_transforms.Compose([\n",
        "                                    torch_transforms.ToTensor(),\n",
        "                                    lambda x:(x>0.5).to(x.dtype)]), #Binarize\n",
        "                                    target_transform=torch_transforms.Compose([\n",
        "                                    lambda x:torch.LongTensor([x.astype(int)]),\n",
        "                                    lambda x:F.one_hot(x-1,len(SUBCLASSES))]),\n",
        "                                    download=True)\n",
        "\n",
        "test_idx = [i for i,y in enumerate(test_dataset.targets) if y in SUBCLASSES]\n",
        "sampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,sampler=sampler)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################# Model Definition #############################\n",
        "VISIBLE_SIZE = DATA_SIZE + LABEL_SIZE\n",
        "HIDDEN_SIZE = 16\n",
        "\n",
        "# Specify model with dimensions\n",
        "rbm = qaml.nn.RBM(VISIBLE_SIZE,HIDDEN_SIZE)\n",
        "\n",
        "# Initialize biases\n",
        "torch.nn.init.uniform_(rbm.b,-0.1,0.1)\n",
        "torch.nn.init.uniform_(rbm.c,-0.1,0.1)\n",
        "torch.nn.init.uniform_(rbm.W,-0.1,0.1)\n",
        "\n",
        "# Set up optimizer\n",
        "optimizer = torch.optim.SGD(rbm.parameters(),lr=learning_rate,\n",
        "                            weight_decay=weight_decay,momentum=momentum)\n",
        "\n",
        "# Trainable inverse temperature\n",
        "beta = torch.nn.Parameter(torch.tensor(2.5), requires_grad=True)\n",
        "# Separate optimizer for inverse temperature\n",
        "beta_optimizer = torch.optim.SGD([beta],lr=0.01)\n",
        "\n",
        "# Set up training mechanisms\n",
        "solver_name = \"Advantage_system1.1\"\n",
        "qa_sampler = qaml.sampler.QASampler(rbm,solver=solver_name,beta=beta)\n",
        "\n",
        "CD = qaml.autograd.SampleBasedConstrastiveDivergence()\n",
        "betaGrad = qaml.autograd.AdaptiveBeta()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################## Model Training ##############################\n",
        "# Set the model to training mode\n",
        "rbm.train()\n",
        "err_log = []\n",
        "scalar_log = []\n",
        "err_beta_log = []\n",
        "accuracy_log = []\n",
        "beta_log = [beta.detach().clone().item()]\n",
        "b_log = [rbm.b.detach().clone().numpy()]\n",
        "c_log = [rbm.c.detach().clone().numpy()]\n",
        "W_log = [rbm.W.detach().clone().numpy().flatten()]\n",
        "for t in range(EPOCHS):\n",
        "    epoch_error = torch.Tensor([0.])\n",
        "    epoch_error_beta = torch.Tensor([0.])\n",
        "\n",
        "    for img_batch, labels_batch in train_loader:\n",
        "        input_data = torch.cat((img_batch.flatten(1),labels_batch.flatten(1)),1)\n",
        "\n",
        "        # Positive Phase\n",
        "        v0,prob_h0 = input_data,rbm(input_data,scale=qa_sampler.beta)\n",
        "\n",
        "        # Negative Phase. Updates scalar\n",
        "        vk, prob_hk = qa_sampler(BATCH_SIZE,auto_scale=True)\n",
        "\n",
        "        # Reconstruction error from Contrastive Divergence\n",
        "        err = CD.apply((v0,prob_h0), (vk,prob_hk), *rbm.parameters())\n",
        "        err_beta = betaGrad.apply(rbm.energy(v0,prob_h0),rbm.energy(vk,prob_hk),beta)\n",
        "\n",
        "        # Do not accumulate gradients\n",
        "        optimizer.zero_grad()\n",
        "        beta_optimizer.zero_grad()\n",
        "\n",
        "        # Compute gradients. Save compute graph at last epoch\n",
        "        err.backward()\n",
        "        err_beta.backward()\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "        # beta_optimizer.step()\n",
        "\n",
        "        #Accumulate error for this epoch\n",
        "        epoch_error  += err\n",
        "        epoch_error_beta  += err_beta\n",
        "\n",
        "    # Error Log\n",
        "    b_log.append(rbm.b.detach().clone().numpy())\n",
        "    c_log.append(rbm.c.detach().clone().numpy())\n",
        "    W_log.append(rbm.W.detach().clone().numpy().flatten())\n",
        "    err_log.append(epoch_error.item())\n",
        "    err_beta_log.append(epoch_error_beta.item())\n",
        "    beta_log.append(beta.detach().clone().item())\n",
        "    scalar_log.append(qa_sampler.scalar)\n",
        "    print(f\"Epoch {t} Reconstruction Error = {epoch_error.item()}\")\n",
        "    print(f\"Alpha = {qa_sampler.scalar}\")\n",
        "    print(f\"Beta = {qa_sampler.beta}\")\n",
        "    print(f\"Effective Beta = {qa_sampler.beta*qa_sampler.scalar}\")\n",
        "    ############################## CLASSIFICATION ##################################\n",
        "    count = 0\n",
        "    for i,(test_data, test_label) in enumerate(test_loader):\n",
        "        prob_hk = rbm(torch.cat((test_data.flatten(1),torch.zeros(1,LABEL_SIZE)),dim=1))\n",
        "        data_pred,label_pred = rbm.generate(prob_hk).split((DATA_SIZE,LABEL_SIZE),dim=1)\n",
        "        if label_pred.argmax() == test_label.argmax():\n",
        "            count+=1\n",
        "    accuracy_log.append(count/len(test_idx))\n",
        "    print(f\"Testing accuracy: {count}/{len(test_idx)} ({count/len(test_idx):.2f})\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############################ MODEL VISUALIZATION ###############################\n",
        "\n",
        "# Testing accuracy graph\n",
        "fig, ax = plt.subplots()\n",
        "plt.plot(accuracy_log)\n",
        "plt.ylabel(\"Testing Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.savefig(\"quantum_accuracy.pdf\")\n",
        "\n",
        "# Beta graph\n",
        "fig, ax = plt.subplots()\n",
        "plt.plot(beta_log)\n",
        "plt.ylabel(\"Beta\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.savefig(\"beta.pdf\")\n",
        "\n",
        "# Beta error graph\n",
        "fig, ax = plt.subplots()\n",
        "plt.plot(err_beta_log)\n",
        "plt.ylabel(\"Beta Error\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.savefig(\"beta_err.pdf\")\n",
        "\n",
        "# Error graph\n",
        "fig, ax = plt.subplots()\n",
        "plt.plot(err_log)\n",
        "plt.ylabel(\"Reconstruction Error\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.savefig(\"quantum_err.pdf\")\n",
        "\n",
        "# Visible bias graph\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_prop_cycle('color', list(plt.get_cmap('turbo',DATA_SIZE).colors))\n",
        "lc_v = ax.plot(b_log)\n",
        "plt.legend(iter(lc_v),[f'b{i}' for i in range(DATA_SIZE)],ncol=2,bbox_to_anchor=(1,1))\n",
        "plt.ylabel(\"Visible Biases\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.savefig(\"quantum_visible_bias_log.pdf\")\n",
        "\n",
        "# Hidden bias graph\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_prop_cycle('color', list(plt.get_cmap('turbo',HIDDEN_SIZE).colors))\n",
        "lc_h = plt.plot(c_log)\n",
        "plt.legend(lc_h,[f'c{i}' for i in range(HIDDEN_SIZE)],ncol=2,bbox_to_anchor=(1,1))\n",
        "plt.ylabel(\"Hidden Biases\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.savefig(\"quantum_hidden_bias_log.pdf\")\n",
        "\n",
        "# Weights graph\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_prop_cycle('color', list(plt.get_cmap('turbo',HIDDEN_SIZE*DATA_SIZE).colors))\n",
        "lc_w = plt.plot(W_log)\n",
        "plt.ylabel(\"Weights\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.savefig(\"quantum_weights_log.pdf\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################## VISUALIZE ###################################\n",
        "plt.matshow(rbm.b.detach()[:DATA_SIZE].view(*SHAPE), cmap='viridis')\n",
        "plt.colorbar()\n",
        "\n",
        "fig,axs = plt.subplots(HIDDEN_SIZE//4,4)\n",
        "for i,ax in enumerate(axs.flat):\n",
        "    weight_matrix = rbm.W[i].detach()[:DATA_SIZE].view(*SHAPE)\n",
        "    ms = ax.matshow(weight_matrix, cmap='viridis', vmin=-1, vmax=1)\n",
        "    ax.axis('off')\n",
        "cbar = fig.colorbar(ms, ax=axs.ravel().tolist(), shrink=0.95)\n",
        "plt.savefig(\"quantum_weights.pdf\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "argv": [
        "C:\\Program Files\\Python39\\python.exe",
        "-m",
        "ipykernel_launcher",
        "-f",
        "{connection_file}"
      ],
      "display_name": "Python 3.9",
      "language": "python",
      "name": "python3.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}