{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# CLassical RBM training on the Bars-And-Stripes Dataset for Reconstruction\n",
    "This is an example on classical Gibbs training of an RBM on the BAS(4,4)\n",
    "dataset.\n",
    "Developed by: Jose Pinilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Required packages\n",
    "import qaml\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as torch_transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "################################# Hyperparameters ##############################\n",
    "M,N = SHAPE = (4,4)\n",
    "DATA_SIZE = N*M\n",
    "HIDDEN_SIZE = 16\n",
    "EPOCHS = 2000\n",
    "SAMPLES = 1000\n",
    "BATCH_SIZE = 500\n",
    "# Stochastic Gradient Descent\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "momentum = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#################################### Input Data ################################\n",
    "train_dataset = qaml.datasets.BAS(*SHAPE,transform=torch_transforms.ToTensor())\n",
    "train_sampler = torch.utils.data.RandomSampler(train_dataset,replacement=True,\n",
    "                                               num_samples=SAMPLES)\n",
    "# Or just shuffle without new samples:\n",
    "# train_sampler = torch.utils.data.RandomSampler(train_dataset,replacement=False)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           sampler=train_sampler,\n",
    "                                           batch_size=BATCH_SIZE)\n",
    "\n",
    "# PLot all data\n",
    "fig,axs = plt.subplots(6,5)\n",
    "for ax,(img,label) in zip(axs.flat,train_dataset):\n",
    "    ax.matshow(img.view(*SHAPE),vmin=0,vmax=1); ax.axis('off')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "################################# Model Definition #############################\n",
    "# Specify model with dimensions\n",
    "rbm = qaml.nn.RBM(DATA_SIZE, HIDDEN_SIZE)\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = torch.optim.SGD(rbm.parameters(), lr=learning_rate,\n",
    "                            weight_decay=weight_decay,\n",
    "                            momentum=momentum)\n",
    "\n",
    "# Set up training mechanisms\n",
    "gibbs_sampler = qaml.sampler.GibbsNetworkSampler(rbm)\n",
    "CD = qaml.autograd.SampleBasedConstrastiveDivergence() # L1 loss\n",
    "# CD = qaml.autograd.ConstrastiveDivergence() # MSE loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "################################## Model Training ##############################\n",
    "# Set the model to training mode\n",
    "rbm.train()\n",
    "err_log = []\n",
    "b_log = [rbm.b.detach().clone().numpy()]\n",
    "c_log = [rbm.c.detach().clone().numpy()]\n",
    "W_log = [rbm.W.detach().clone().numpy().flatten()]\n",
    "for t in range(EPOCHS):\n",
    "    epoch_error = torch.Tensor([0.])\n",
    "    for img_batch, labels_batch in train_loader:\n",
    "        input_data = img_batch.flatten(1)\n",
    "\n",
    "        # Positive Phase\n",
    "        v0, prob_h0 = input_data, rbm(input_data)\n",
    "        # Negative Phase\n",
    "        vk, prob_hk = gibbs_sampler(v0.detach(), k=5)\n",
    "        # Or sample from random init\n",
    "        # vk, prob_hk = gibbs_sampler(0.1*torch.randn((500,rbm.V)), k=5)\n",
    "\n",
    "        # Reconstruction error from Contrastive Divergence\n",
    "        err = CD.apply((v0,prob_h0), (vk,prob_hk), *rbm.parameters())\n",
    "\n",
    "        # Do not accumulated gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute gradients. Save compute graph at last epoch\n",
    "        err.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        #Accumulate error for this epoch\n",
    "        epoch_error  += err\n",
    "\n",
    "    # Error Log\n",
    "    b_log.append(rbm.b.detach().clone().numpy())\n",
    "    c_log.append(rbm.c.detach().clone().numpy())\n",
    "    W_log.append(rbm.W.detach().clone().numpy().flatten())\n",
    "    err_log.append(epoch_error.item())\n",
    "    print(f\"Epoch {t} Reconstruction Error = {epoch_error.item()}\")\n",
    "# Set the model to evaluation mode\n",
    "# rbm.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "################################## Sampling ####################################\n",
    "N = 1000\n",
    "prob_v,_ = gibbs_sampler(torch.rand(N,DATA_SIZE),k=10)\n",
    "img_samples = prob_v.view(N,*SHAPE).bernoulli()\n",
    "# PLot some samples\n",
    "fig,axs = plt.subplots(4,5)\n",
    "for ax,img in zip(axs.flat,img_samples):\n",
    "    ax.matshow(img.view(*SHAPE),vmin=0,vmax=1); ax.axis('off')\n",
    "plt.tight_layout()\n",
    "# Get and print score\n",
    "p,r,score = train_dataset.score(img_samples)\n",
    "print(f\"qBAS : Precision = {p:.02} Recall = {r:.02} Score = {score:.02}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "############################## RECONSTRUCTION ##################################\n",
    "k = 10\n",
    "count = 0\n",
    "\n",
    "mask = torch_transforms.functional.erase(torch.ones(1,*SHAPE),i=1,j=1,h=2,w=2,v=0).flatten()\n",
    "for img, label in train_dataset:\n",
    "\n",
    "    clamped = mask*img.flatten(1)\n",
    "    prob_hk = rbm.forward(clamped + (1-mask)*0.5)\n",
    "    prob_vk = rbm.generate(prob_hk).detach()\n",
    "    for _ in range(k):\n",
    "        masked = clamped + (1-mask)*prob_vk.data\n",
    "        prob_hk.data = rbm.forward(masked).data\n",
    "        prob_vk.data = rbm.generate(prob_hk).data\n",
    "    recon = (clamped + (1-mask)*prob_vk).bernoulli().view(img.shape)\n",
    "\n",
    "    if recon.equal(img):\n",
    "        count+=1\n",
    "print(f\"Dataset Reconstruction: {count/len(train_dataset):.02}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "############################ MODEL VISUALIZATION ###############################\n",
    "\n",
    "# L1 error graph\n",
    "plt.plot(err_log)\n",
    "plt.ylabel(\"Reconstruction Error (L1)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.savefig(\"classical_err_log.pdf\")\n",
    "\n",
    "# Visible bias graph\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_prop_cycle('color', list(plt.get_cmap('turbo',DATA_SIZE).colors))\n",
    "lc_v = ax.plot(b_log)\n",
    "plt.legend(lc_v,[f'b{i}' for i in range(DATA_SIZE)],ncol=4,loc=(0,1))\n",
    "plt.ylabel(\"Visible Biases\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.savefig(\"classival_b_log.pdf\")\n",
    "\n",
    "# Hidden bias graph\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_prop_cycle('color', list(plt.get_cmap('turbo',HIDDEN_SIZE).colors))\n",
    "lc_h = plt.plot(c_log)\n",
    "plt.legend(lc_h,[f'c{i}' for i in range(HIDDEN_SIZE)],ncol=4,loc=(0,1))\n",
    "plt.ylabel(\"Hidden Biases\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.savefig(\"classical_c_log.pdf\")\n",
    "\n",
    "# Weights graph\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_prop_cycle('color', list(plt.get_cmap('turbo',rbm.V*rbm.H).colors))\n",
    "lc_w = plt.plot(W_log)\n",
    "labels = [f'W{i}' for i in range(rbm.V*rbm.H)]\n",
    "plt.legend(lc_w,labels,ncol=10,bbox_to_anchor=(1,1))\n",
    "plt.ylabel(\"Weights\")\n",
    "plt.xlabel(\"Epoch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "################################## ENERGY ######################################\n",
    "\n",
    "data_energies = []\n",
    "for img,label in train_dataset:\n",
    "    data = img.flatten(1)\n",
    "    data_energies.append(rbm.free_energy(data).item())\n",
    "\n",
    "rand_energies = []\n",
    "rand_data = torch.rand(len(train_dataset)*10,rbm.V)\n",
    "for img in rand_data:\n",
    "    rand_energies.append(rbm.free_energy(img.bernoulli()).item())\n",
    "\n",
    "gibbs_energies = []\n",
    "gibbs_sampler = qaml.sampler.GibbsNetworkSampler(rbm)\n",
    "for img,label in train_dataset:\n",
    "    data = img.flatten(1)\n",
    "    prob_v,prob_h = gibbs_sampler(data,k=5)\n",
    "    gibbs_energies.append(rbm.free_energy(prob_v.bernoulli()).item())\n",
    "\n",
    "\n",
    "qa_energies = []\n",
    "solver_name = \"Advantage_system1.1\"\n",
    "qa_sampler = qaml.sampler.QuantumAnnealingNetworkSampler(rbm,solver=solver_name)\n",
    "qa_sampleset = qa_sampler(num_reads=1000)\n",
    "for s_v,s_h in zip(*qa_sampleset):\n",
    "    qa_energies.append(rbm.free_energy(s_v.detach()).item())\n",
    "\n",
    "plot_data = [(data_energies,  'Data',    'blue'),\n",
    "             (rand_energies,  'Random',  'red'),\n",
    "             (gibbs_energies, 'Gibbs-5', 'green'),\n",
    "             (qa_energies,    'Quantum', 'orange')]\n",
    "\n",
    "hist_kwargs = {'ec':'k','lw':2.0,'alpha':0.5,'histtype':'stepfilled','bins':100}\n",
    "weights = lambda data: [1./len(data) for _ in data]\n",
    "\n",
    "for data,name,color in plot_data:\n",
    "    plt.hist(data,weights=weights(data),label=name,color=color,**hist_kwargs)\n",
    "\n",
    "plt.xlabel(\"Energy\")\n",
    "plt.ylabel(\"Count/Total\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig(\"classical_energies.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "################################## VISUALIZE ###################################\n",
    "plt.matshow(rbm.b.detach().view(*SHAPE), cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.savefig(\"classical_b.pdf\")\n",
    "\n",
    "fig,axs = plt.subplots(HIDDEN_SIZE//4,4)\n",
    "for i,ax in enumerate(axs.flat):\n",
    "    weight_matrix = rbm.W[i].detach().view(*SHAPE)\n",
    "    ms = ax.matshow(weight_matrix, cmap='viridis')\n",
    "    ax.axis('off')\n",
    "fig.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "cbar = fig.colorbar(ms, ax=axs.ravel().tolist(), shrink=0.95)\n",
    "plt.savefig(\"classical_weights.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm(torch.randn((1,rbm.V)),scale=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
