{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Quantum-Assisted RBM training on the BAS Dataset for Reconstruction\n",
        "This is an example on quantum-assisted training of an RBM on the BAS(4,4)\n",
        "dataset.\n",
        "Developed by: Jose Pinilla"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Required packages\n",
        "import qaml\n",
        "import torch\n",
        "torch.manual_seed(0) # For deterministic weights\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as torch_transforms\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################# Hyperparameters ##############################\n",
        "M,N = SHAPE = (4,4)\n",
        "DATA_SIZE = N*M\n",
        "HIDDEN_SIZE = 16\n",
        "EPOCHS = 75\n",
        "SAMPLES = 1000\n",
        "BATCH_SIZE = 500\n",
        "# Stochastic Gradient Descent\n",
        "learning_rate = 0.1\n",
        "weight_decay = 1e-4\n",
        "momentum = 0.5\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#################################### Input Data ################################\n",
        "train_dataset = qaml.datasets.BAS(*SHAPE,transform=torch_transforms.ToTensor())\n",
        "train_sampler = torch.utils.data.RandomSampler(train_dataset,replacement=True,\n",
        "                                               num_samples=SAMPLES)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,sampler=train_sampler,\n",
        "                                           batch_size=BATCH_SIZE)\n",
        "\n",
        "# PLot all data\n",
        "fig,axs = plt.subplots(6,5)\n",
        "for ax,(img,label) in zip(axs.flat,train_dataset):\n",
        "    ax.matshow(img.view(*SHAPE),vmin=0,vmax=1); ax.axis('off')\n",
        "plt.tight_layout()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################# Model Definition #############################\n",
        "# Trainable inverse temperature\n",
        "beta = torch.nn.Parameter(torch.tensor(1.5), requires_grad=True)\n",
        "# Specify model with dimensions\n",
        "rbm = qaml.nn.RBM(DATA_SIZE,HIDDEN_SIZE,beta=beta)\n",
        "\n",
        "# Initialize biases\n",
        "torch.nn.init.uniform_(rbm.b,-0.1,0.1)\n",
        "torch.nn.init.uniform_(rbm.c,-0.1,0.1)\n",
        "torch.nn.init.uniform_(rbm.W,-0.1,0.1)\n",
        "\n",
        "# Set up optimizers\n",
        "optimizer = torch.optim.SGD(rbm.parameters(), lr=learning_rate,\n",
        "                            weight_decay=weight_decay,momentum=momentum)\n",
        "# Separate optimizer for inverse temperature\n",
        "beta_optimizer = torch.optim.SGD([beta],lr=0.01)\n",
        "\n",
        "# Set up training mechanisms\n",
        "solver_name = \"Advantage_system1.1\"\n",
        "qa_sampler = qaml.sampler.QuantumAnnealingNetworkSampler(rbm,solver=solver_name)\n",
        "CD = qaml.autograd.SampleBasedConstrastiveDivergence()\n",
        "betaGrad = qaml.autograd.AdaptiveBeta()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################## Model Training ##############################\n",
        "# Set the model to training mode\n",
        "rbm.train()\n",
        "err_log = []\n",
        "scalar_log = []\n",
        "err_beta_log = []\n",
        "beta_log = [beta.item()]\n",
        "b_log = [rbm.b.detach().numpy()]\n",
        "c_log = [rbm.c.detach().numpy()]\n",
        "W_log = [rbm.W.detach().numpy().flatten()]\n",
        "for t in range(EPOCHS):\n",
        "    epoch_error = torch.Tensor([0.])\n",
        "    epoch_error_beta = torch.Tensor([0.])\n",
        "    for img_batch, labels_batch in train_loader:\n",
        "        input_data = img_batch.flatten(1)\n",
        "\n",
        "        # Positive Phase\n",
        "        v0, prob_h0 = input_data, rbm(input_data)\n",
        "        # Negative Phase\n",
        "        vk, prob_hk = qa_sampler(BATCH_SIZE,auto_scale=True)\n",
        "\n",
        "        # Reconstruction error from Contrastive Divergence\n",
        "        err = CD.apply((v0,prob_h0), (vk,prob_hk), *rbm.parameters())\n",
        "        err_beta = betaGrad.apply(rbm.energy(v0,prob_h0),rbm.energy(vk,prob_hk),beta) #TODO: use sampleset Energies?\n",
        "\n",
        "        # Do not accumulate gradients\n",
        "        optimizer.zero_grad()\n",
        "        beta_optimizer.zero_grad()\n",
        "\n",
        "        # Compute gradients. Save compute graph at last epoch\n",
        "        err.backward()\n",
        "        err_beta.backward()\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "        beta_optimizer.step()\n",
        "\n",
        "        #Accumulate error for this epoch\n",
        "        epoch_error  += err\n",
        "        epoch_error_beta  += err_beta\n",
        "\n",
        "    # Error Log\n",
        "    beta_log.append(beta.item())\n",
        "    b_log.append(rbm.b.detach().numpy())\n",
        "    c_log.append(rbm.c.detach().numpy())\n",
        "    W_log.append(rbm.W.detach().numpy().flatten())\n",
        "    err_log.append(epoch_error.item())\n",
        "    scalar_log.append(qa_sampler.scalar)\n",
        "    err_beta_log.append(epoch_error_beta.item())\n",
        "    print(f\"Epoch {t} Reconstruction Error = {epoch_error.item()}\")\n",
        "    print(f\"Beta = {rbm.beta}\")\n",
        "    print(f\"Alpha = {qa_sampler.scalar}\")\n",
        "    print(f\"Effective Beta = {rbm.beta*qa_sampler.scalar}\")\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "# rbm.eval()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################# qBAS Score ###################################\n",
        "N = 1000 # CLASSICAL\n",
        "gibbs_sampler = qaml.sampler.GibbsNetworkSampler(rbm)\n",
        "prob_v,_ = gibbs_sampler(torch.rand(N,DATA_SIZE),k=10)\n",
        "img_samples = prob_v.view(N,*SHAPE).bernoulli()\n",
        "# PLot some samples\n",
        "fig,axs = plt.subplots(4,5)\n",
        "for ax,img in zip(axs.flat,img_samples):\n",
        "    ax.matshow(img.view(*SHAPE),vmin=0,vmax=1); ax.axis('off')\n",
        "plt.tight_layout()\n",
        "# Get and print score\n",
        "p,r,score = train_dataset.score(img_samples)\n",
        "print(f\"qBAS : Precision = {p:.02} Recall = {r:.02} Score = {score:.02}\")\n",
        "\n",
        "############################## RECONSTRUCTION ##################################\n",
        "k = 10\n",
        "count = 0\n",
        "mask = torch_transforms.functional.erase(torch.ones(SHAPE),1,1,2,2,0).flatten()\n",
        "for img, label in train_dataset:\n",
        "\n",
        "    clamped = mask*img.flatten(1)\n",
        "    prob_hk = rbm.forward(clamped + (1-mask)*0.5)\n",
        "    prob_vk = rbm.generate(prob_hk).detach()\n",
        "    for _ in range(k):\n",
        "        masked = clamped + (1-mask)*prob_vk.data\n",
        "        prob_hk.data = rbm.forward(masked).data\n",
        "        prob_vk.data = rbm.generate(prob_hk).data\n",
        "    recon = (clamped + (1-mask)*prob_vk).bernoulli().view(img.shape)\n",
        "\n",
        "    if recon.equal(img):\n",
        "        count+=1\n",
        "\n",
        "print(f\"Dataset Reconstruction: {count/len(train_dataset):.02}\")\n",
        "\n",
        "############################ MODEL VISUALIZATION ###############################\n",
        "\n",
        "# Beta graph\n",
        "plt.plot(beta_log)\n",
        "plt.ylabel(\"Beta\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "\n",
        "# Scalar graph\n",
        "plt.plot(scalar_log)\n",
        "plt.ylabel(\"Scalar\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "\n",
        "# Beta error graph\n",
        "plt.plot(err_beta_log)\n",
        "plt.ylabel(\"Beta Error\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "\n",
        "# L1 error graph\n",
        "plt.plot(err_log)\n",
        "plt.ylabel(\"Reconstruction Error\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.savefig(\"quantum_err_log.pdf\")\n",
        "\n",
        "# Visible bias graph\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_prop_cycle('color', list(plt.get_cmap('turbo',DATA_SIZE).colors))\n",
        "lc_v = ax.plot(b_log)\n",
        "plt.legend(iter(lc_v),[f'b{i}' for i in range(DATA_SIZE)],ncol=2,bbox_to_anchor=(1,1))\n",
        "plt.ylabel(\"Visible Biases\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.savefig(\"quantum_b_log.pdf\")\n",
        "\n",
        "# Hidden bias graph\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_prop_cycle('color', list(plt.get_cmap('turbo',HIDDEN_SIZE).colors))\n",
        "lc_h = plt.plot(c_log)\n",
        "plt.legend(lc_h,[f'c{i}' for i in range(HIDDEN_SIZE)],ncol=2,bbox_to_anchor=(1,1))\n",
        "plt.ylabel(\"Hidden Biases\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.savefig(\"quantum_c_log.pdf\")\n",
        "\n",
        "# Weights graph\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_prop_cycle('color', list(plt.get_cmap('turbo',rbm.V*rbm.H).colors))\n",
        "lc_w = plt.plot(W_log)\n",
        "plt.legend(lc_w,[f'W{i}' for i in range(rbm.V*rbm.H)],ncol=10,bbox_to_anchor=(1,1))\n",
        "plt.ylabel(\"Weights\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "\n",
        "################################## ENERGY ######################################\n",
        "rand_data = torch.rand(len(train_dataset)*10,rbm.V)\n",
        "rand_energies = rbm.free_energy(rand_data.bernoulli()).detach().numpy()\n",
        "\n",
        "data_energies = []\n",
        "for img,label in train_dataset:\n",
        "    data = img.flatten(1)\n",
        "    data_energies.append(rbm.free_energy(data).item())\n",
        "\n",
        "gibbs_energies = []\n",
        "gibbs_sampler = qaml.sampler.GibbsNetworkSampler(rbm)\n",
        "for img,label in train_dataset:\n",
        "    data = img.flatten(1)\n",
        "    prob_v,prob_h = gibbs_sampler(data,k=5)\n",
        "    gibbs_energies.append(rbm.free_energy(prob_v.bernoulli()).item())\n",
        "\n",
        "qa_energies = []\n",
        "qa_sampleset = qa_sampler(num_reads=1000,auto_scale=True)\n",
        "for s_v,s_h in zip(*qa_sampleset):\n",
        "    qa_energies.append(rbm.free_energy(s_v.detach()).item())\n",
        "\n",
        "\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "hist_kwargs = {'ec':'k','lw':2.0,'alpha':0.5,'histtype':'stepfilled','bins':100}\n",
        "matplotlib.rcParams.update({'font.size': 22})\n",
        "weights = lambda data: np.ones_like(data)/len(data)\n",
        "\n",
        "plt.hist(rand_energies,weights=weights(rand_energies),label=\"Random\",color='r',**hist_kwargs)\n",
        "plt.hist(data_energies,weights=weights(data_energies), label=\"Data\", color='b', **hist_kwargs)\n",
        "plt.hist(gibbs_energies,weights=weights(gibbs_energies),label=\"Gibbs-1\",color='g',**hist_kwargs)\n",
        "plt.hist(qa_energies,weights=weights(qa_energies),label=\"QA\",color='orange', **hist_kwargs)\n",
        "\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylim(0.0,0.05)\n",
        "plt.ylabel(\"Count/Total\")\n",
        "plt.xlabel(\"Energy\")\n",
        "plt.savefig(\"quantum_energies.pdf\")\n",
        "\n",
        "################################## VISUALIZE ###################################\n",
        "plt.matshow(rbm.b.detach().view(*SHAPE), cmap='viridis')\n",
        "plt.colorbar()\n",
        "\n",
        "fig,axs = plt.subplots(HIDDEN_SIZE//4,4)\n",
        "for i,ax in enumerate(axs.flat):\n",
        "    weight_matrix = rbm.W[i].detach().view(*SHAPE)\n",
        "    ms = ax.matshow(weight_matrix, cmap='viridis', vmin=-1, vmax=1)\n",
        "    ax.axis('off')\n",
        "fig.subplots_adjust(wspace=0.1, hspace=0.1)\n",
        "cbar = fig.colorbar(ms, ax=axs.ravel().tolist(), shrink=0.95)\n",
        "plt.savefig(\"quantum_weights.pdf\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "argv": [
        "C:\\Program Files\\Python39\\python.exe",
        "-m",
        "ipykernel_launcher",
        "-f",
        "{connection_file}"
      ],
      "display_name": "Python 3.9",
      "language": "python",
      "name": "python3.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}